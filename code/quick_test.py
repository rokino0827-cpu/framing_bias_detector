"""
å¿«é€Ÿæµ‹è¯•è„šæœ¬ - éªŒè¯æ¨¡å‹åŸºæœ¬åŠŸèƒ½
"""

import os
import sys
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

import torch
import pandas as pd
from transformers import AutoTokenizer

from framing_bias_detector.config import default_config
from framing_bias_detector.model import LongformerBiasDetector
from framing_bias_detector.data_processor import ArticlePreprocessor
from framing_bias_detector.utils import setup_environment, get_device_info, log_system_info

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    logger.info("æµ‹è¯•æ¨¡å‹åŠ è½½...")
    
    try:
        # ä½¿ç”¨è¾ƒå°çš„é…ç½®è¿›è¡Œæµ‹è¯•
        config = default_config
        config.model.max_length = 512  # å‡å°é•¿åº¦ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
        
        model = LongformerBiasDetector(config)
        logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # è·å–æ¨¡å‹å¤§å°ä¿¡æ¯
        model_size = model.get_model_size()
        logger.info(f"æ¨¡å‹å‚æ•°ç»Ÿè®¡: {model_size}")
        
        return model
        
    except Exception as e:
        logger.error(f"âœ— æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        return None

def test_data_preprocessing():
    """æµ‹è¯•æ•°æ®é¢„å¤„ç†"""
    logger.info("æµ‹è¯•æ•°æ®é¢„å¤„ç†...")
    
    try:
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = pd.DataFrame({
            'content': [
                "This is a test article about political bias detection.",
                "Another article discussing media bias and framing effects.",
                "A neutral article about technology and innovation."
            ],
            'publication': ['Test News', 'Example Media', 'Tech Today'],
            'bias_probability': [0.8, 0.6, 0.2],
            'confidence_score': [0.9, 0.7, 0.8]
        })
        
        # æµ‹è¯•é¢„å¤„ç†å™¨
        preprocessor = ArticlePreprocessor(
            tokenizer_name=default_config.model.longformer_model,
            max_length=default_config.model.max_length
        )
        processed_data = preprocessor.preprocess_dataframe(test_data)
        
        logger.info(f"âœ“ æ•°æ®é¢„å¤„ç†æˆåŠŸ - å¤„ç†äº† {len(processed_data)} æ¡è®°å½•")
        return processed_data
        
    except Exception as e:
        logger.error(f"âœ— æ•°æ®é¢„å¤„ç†å¤±è´¥: {str(e)}")
        return None

def test_model_forward():
    """æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­"""
    logger.info("æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­...")
    
    try:
        config = default_config
        config.model.max_length = 512
        
        model = LongformerBiasDetector(config)
        model.eval()
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        tokenizer = AutoTokenizer.from_pretrained(config.model.longformer_model)
        
        test_text = "This is a test article for bias detection."
        inputs = tokenizer(
            test_text,
            max_length=config.model.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = model(
                input_ids=inputs['input_ids'],
                attention_mask=inputs['attention_mask']
            )
        
        logger.info("âœ“ æ¨¡å‹å‰å‘ä¼ æ’­æˆåŠŸ")
        logger.info(f"è¾“å‡ºå½¢çŠ¶ - logits: {outputs['logits'].shape}, confidence: {outputs['confidence'].shape}")
        
        return True
        
    except Exception as e:
        logger.error(f"âœ— æ¨¡å‹å‰å‘ä¼ æ’­å¤±è´¥: {str(e)}")
        return False

def test_prediction():
    """æµ‹è¯•é¢„æµ‹åŠŸèƒ½"""
    logger.info("æµ‹è¯•é¢„æµ‹åŠŸèƒ½...")
    
    try:
        config = default_config
        config.model.max_length = 512
        
        model = LongformerBiasDetector(config)
        model.eval()
        
        tokenizer = AutoTokenizer.from_pretrained(config.model.longformer_model)
        
        test_texts = [
            "This article presents a balanced view of the political situation.",
            "The government's policies are clearly biased and unfair to citizens.",
            "Technology companies are revolutionizing the way we work."
        ]
        
        for i, text in enumerate(test_texts):
            inputs = tokenizer(
                text,
                max_length=config.model.max_length,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            
            predictions, probabilities, confidence = model.predict_bias(
                inputs['input_ids'],
                inputs['attention_mask']
            )
            
            logger.info(f"æ–‡æœ¬ {i+1}:")
            logger.info(f"  é¢„æµ‹: {predictions.item()}")
            logger.info(f"  æ¦‚ç‡: {probabilities[0].tolist()}")
            logger.info(f"  ç½®ä¿¡åº¦: {confidence.item():.3f}")
        
        logger.info("âœ“ é¢„æµ‹åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        logger.error(f"âœ— é¢„æµ‹åŠŸèƒ½æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

def test_gpu_availability():
    """æµ‹è¯•GPUå¯ç”¨æ€§"""
    logger.info("æµ‹è¯•GPUå¯ç”¨æ€§...")
    
    device_info = get_device_info()
    
    if device_info['cuda_available']:
        logger.info("âœ“ CUDAå¯ç”¨")
        logger.info(f"GPUæ•°é‡: {device_info['device_count']}")
        
        for device in device_info['devices']:
            logger.info(f"  GPU {device['id']}: {device['name']}")
            logger.info(f"    æ€»æ˜¾å­˜: {device['memory_total'] / 1024**3:.1f} GB")
        
        # æµ‹è¯•GPUä¸Šçš„ç®€å•è®¡ç®—
        try:
            x = torch.randn(100, 100).cuda()
            y = torch.mm(x, x.t())
            logger.info("âœ“ GPUè®¡ç®—æµ‹è¯•æˆåŠŸ")
        except Exception as e:
            logger.warning(f"GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {str(e)}")
        
        return True
    else:
        logger.warning("CUDAä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨CPU")
        return False

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    logger.info("å¼€å§‹è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
    
    # è®¾ç½®ç¯å¢ƒ
    setup_environment()
    log_system_info()
    
    test_results = {}
    
    # æµ‹è¯•GPU
    test_results['gpu'] = test_gpu_availability()
    
    # æµ‹è¯•æ•°æ®é¢„å¤„ç†
    test_results['data_preprocessing'] = test_data_preprocessing() is not None
    
    # æµ‹è¯•æ¨¡å‹åŠ è½½
    test_results['model_loading'] = test_model_loading() is not None
    
    # æµ‹è¯•å‰å‘ä¼ æ’­
    test_results['forward_pass'] = test_model_forward()
    
    # æµ‹è¯•é¢„æµ‹
    test_results['prediction'] = test_prediction()
    
    # æ€»ç»“ç»“æœ
    logger.info("\n" + "="*50)
    logger.info("æµ‹è¯•ç»“æœæ€»ç»“:")
    logger.info("="*50)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results.items():
        status = "âœ“ é€šè¿‡" if result else "âœ— å¤±è´¥"
        logger.info(f"{test_name:20s}: {status}")
        if result:
            passed += 1
    
    logger.info("="*50)
    logger.info(f"æ€»è®¡: {passed}/{total} æµ‹è¯•é€šè¿‡")
    
    if passed == total:
        logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹åŸºæœ¬åŠŸèƒ½æ­£å¸¸")
        return True
    else:
        logger.warning(f"âš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒé…ç½®")
        return False

if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
